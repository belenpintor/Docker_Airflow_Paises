[2023-07-25 23:37:48,435] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: dag_con_conexion_postgres.crear_tabla_ manual__2023-07-25T23:37:42.407563+00:00 [queued]>
[2023-07-25 23:37:48,474] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: dag_con_conexion_postgres.crear_tabla_ manual__2023-07-25T23:37:42.407563+00:00 [queued]>
[2023-07-25 23:37:48,482] {taskinstance.py:1376} INFO - 
--------------------------------------------------------------------------------
[2023-07-25 23:37:48,490] {taskinstance.py:1377} INFO - Starting attempt 1 of 1
[2023-07-25 23:37:48,496] {taskinstance.py:1378} INFO - 
--------------------------------------------------------------------------------
[2023-07-25 23:37:48,554] {taskinstance.py:1397} INFO - Executing <Task(PostgresOperator): crear_tabla_> on 2023-07-25 23:37:42.407563+00:00
[2023-07-25 23:37:48,567] {standard_task_runner.py:52} INFO - Started process 223 to run task
[2023-07-25 23:37:48,579] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_con_conexion_postgres', 'crear_tabla_', 'manual__2023-07-25T23:37:42.407563+00:00', '--job-id', '46', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_paises.py', '--cfg-path', '/tmp/tmpne76uyoh', '--error-file', '/tmp/tmpxsx13adu']
[2023-07-25 23:37:48,584] {standard_task_runner.py:80} INFO - Job 46: Subtask crear_tabla_
[2023-07-25 23:37:48,870] {task_command.py:371} INFO - Running <TaskInstance: dag_con_conexion_postgres.crear_tabla_ manual__2023-07-25T23:37:42.407563+00:00 [running]> on host 484a5a9deba0
[2023-07-25 23:37:49,238] {taskinstance.py:1591} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Belenpintor
AIRFLOW_CTX_DAG_ID=dag_con_conexion_postgres
AIRFLOW_CTX_TASK_ID=crear_tabla_
AIRFLOW_CTX_EXECUTION_DATE=2023-07-25T23:37:42.407563+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-07-25T23:37:42.407563+00:00
[2023-07-25 23:37:49,282] {base.py:68} INFO - Using connection ID 'redshift_belen' for task execution.
[2023-07-25 23:37:51,085] {dbapi.py:231} INFO - Running statement: 
CREATE TABLE IF NOT EXISTS bapintor_coderhouse.ciudades (
    city VARCHAR PRIMARY KEY,
    "Housing" DECIMAL(10, 2),
    "Cost of Living" DECIMAL(10, 2),
    "Startups" DECIMAL(10, 2),
    "Venture Capital" DECIMAL(10, 2),
    "Travel Connectivity" DECIMAL(10, 2),
    "Commute" DECIMAL(10, 2),
    "Business Freedom" DECIMAL(10, 2),
    "Safety" DECIMAL(10, 2),
    "Healthcare" DECIMAL(10, 2),
    "Education" DECIMAL(10, 2),
    "Environmental Quality" DECIMAL(10, 2),
    "Economy" DECIMAL(10, 2),
    "Taxation" DECIMAL(10, 2),
    "Internet Access" DECIMAL(10, 2),
    "Leisure & Culture" DECIMAL(10, 2),
    "Tolerance" DECIMAL(10, 2),
    "Outdoors" DECIMAL(10, 2),
    "process_date" TIMESTAMP DISTKEY
);, parameters: None
[2023-07-25 23:37:51,243] {taskinstance.py:1909} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/postgres/operators/postgres.py", line 92, in execute
    self.hook.run(self.sql, self.autocommit, parameters=self.parameters)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/hooks/dbapi.py", line 211, in run
    self._run_command(cur, sql_statement, parameters)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/hooks/dbapi.py", line 235, in _run_command
    cur.execute(sql_statement)
psycopg2.errors.InvalidSchemaName: schema "bapintor_coderhouse" does not exist

[2023-07-25 23:37:51,446] {taskinstance.py:1420} INFO - Marking task as FAILED. dag_id=dag_con_conexion_postgres, task_id=crear_tabla_, execution_date=20230725T233742, start_date=20230725T233748, end_date=20230725T233751
[2023-07-25 23:37:51,621] {standard_task_runner.py:97} ERROR - Failed to execute job 46 for task crear_tabla_ (schema "bapintor_coderhouse" does not exist
; 223)
[2023-07-25 23:37:51,829] {local_task_job.py:156} INFO - Task exited with return code 1
[2023-07-25 23:37:52,629] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
